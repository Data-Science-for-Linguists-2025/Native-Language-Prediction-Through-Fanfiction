{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# file set-up\n",
    "1. import necessary libraries  \n",
    "2. title directory (where data files are stored)  \n",
    "3. create empty dataframe  \n",
    "4. create list of languages\n",
    "    will return to this---currently it doesn't actually serve a purpose, as I was unable to get the languages through here.  \n",
    "    will either update this file or change spider to sort by language.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as beau\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import pickle\n",
    "import nltk\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'natfinder/natfinder/scraped-works'\n",
    "fics_df = pd.DataFrame(columns=['L1', 'author', 'title', 'tags', 'summary', 'notes', 'endnotes', 'work', 'rating', 'warnings', 'category', 'fandom', 'ships', 'charas', 'freeform', 'published', 'status', 'word_count', 'chaps', 'comments', 'kudos', 'hits', 'bookmarks'])\n",
    "langs = ['french', 'spanish', 'mexican', 'brazilian', 'portugeuse', 'russian', 'ukranian', 'bengali', 'italian', 'czech', 'japanese', 'korean', 'chinese', 'swedish', 'german', 'finnish', 'turkish', 'greek', 'hindi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# def: extract_info(soup)\n",
    "1. try/except in case data is missing from file (ie; fics written by anonymous users will not return an author value)  \n",
    "2. gather basic info (author, title, tags, summary, notes, endnotes, work)  \n",
    "    tag info (rating, warning, category, fandom, (relation)ships, characters, freeform)  \n",
    "    stat info (publication date, status, word count, chapters, comments, kudos, hits, bookmarks)\n",
    "3. add to `fics_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(soup):\n",
    "    # basic info\n",
    "    try:\n",
    "        author = soup.find(rel='author').text\n",
    "    except:\n",
    "        author = 'NaN'\n",
    "    try:\n",
    "        title = soup.find(class_='title heading').text\n",
    "    except:\n",
    "        title = 'NaN'\n",
    "    try:\n",
    "        tags = [t.text for t in soup.find_all(class_='tag')]\n",
    "    except:\n",
    "        tags = 'NaN'\n",
    "    try:\n",
    "        summary = soup.find('div', class_='summary module').text\n",
    "    except:\n",
    "        summary = 'NaN'\n",
    "    try:\n",
    "        notes = soup.find('div', class_='notes module').text\n",
    "    except:\n",
    "        notes = 'NaN'\n",
    "    try:\n",
    "        endnotes = soup.find('div', class_='end notes module').text\n",
    "    except:\n",
    "        endnotes = 'NaN'\n",
    "    try:\n",
    "        work = soup.text\n",
    "    except:\n",
    "        work = 'NaN'\n",
    "    # tag info\n",
    "    try:\n",
    "        rating = [[a.text for a in t.find_all('a')] for t in soup.find_all(class_='rating tags')]\n",
    "    except:\n",
    "        rating = 'ERROR'\n",
    "    try:\n",
    "        warnings = [[a.text for a in t.find_all('a')] for t in soup.find_all(class_='warning tags')]\n",
    "    except:\n",
    "        warnings = 'ERROR'\n",
    "    try:\n",
    "        categories = [[a.text for a in t.find_all('a')] for t in soup.find_all(class_='category tags')]\n",
    "    except:\n",
    "        categories = 'ERROR'\n",
    "    try:\n",
    "        fandoms = [[a.text for a in t.find_all('a')] for t in soup.find_all(class_='fandom tags')]\n",
    "    except:\n",
    "        fandoms = 'ERROR'\n",
    "    try:\n",
    "        ships = [[a.text for a in t.find_all('a')] for t in soup.find_all(class_='relationship tags')]\n",
    "    except:\n",
    "        ships = 'NaN'\n",
    "    try:\n",
    "        charas = [[a.text for a in t.find_all('a')] for t in soup.find_all(class_='character tags')]\n",
    "    except:\n",
    "        charas = 'NaN'\n",
    "    try:\n",
    "        freeform = [[a.text for a in t.find_all('a')] for t in soup.find_all(class_='freeform tags')]\n",
    "    except:\n",
    "        freeform = 'NaN'\n",
    "    # stats info\n",
    "    try:\n",
    "        published = soup.dd.find(class_='published').text\n",
    "    except:\n",
    "        published = 'ERROR'\n",
    "    try:\n",
    "        status = soup.dd.find(class_='status').text\n",
    "    except:\n",
    "        status = 'ERROR'\n",
    "    try:\n",
    "        wc = int(soup.dd.find(class_='words').text)\n",
    "    except:\n",
    "        wc = 'ERROR'\n",
    "    try:\n",
    "        chapters = int(soup.dd.find(class_='chapters').text)\n",
    "    except:\n",
    "        chapters = 'ERROR'\n",
    "    try:\n",
    "        comments = int(soup.dd.find(class_='comments').text)\n",
    "    except:\n",
    "        comments = 0\n",
    "    try:\n",
    "        kudos = int(soup.dd.find(class_='kudos').text)\n",
    "    except:\n",
    "        kudos = 0\n",
    "    try:\n",
    "        hits = int(soup.dd.find(class_='hits').text)\n",
    "    except:\n",
    "        hits = 0\n",
    "    try:\n",
    "        bookmarks = int(soup.dd.find(class_='bookmarks').text)\n",
    "    except:\n",
    "        bookmarks = 0\n",
    "    temp = pd.DataFrame({'author':[author], 'title':[title],'tags':[tags], 'summary':[summary], 'notes':[notes], 'endnotes':[endnotes], 'work':[work], 'rating':[rating], 'warnings':[warnings], 'category':[categories], 'fandom':[fandoms], 'ships':[ships], 'charas':[charas], 'freeform':[freeform], 'published':[published], 'status':[status], 'word_count':[wc], 'chaps':[chapters], 'comments':[comments], 'kudos':[kudos], 'hits':[hits], 'bookmarks':[bookmarks]})\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parsing through files\n",
    "1. gather files  \n",
    "2. create bs4 soup for files\n",
    "3. use `extract_info(soup)` to add to fics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for f in os.scandir(directory):\n",
    "    if f.name.endswith('.html'):\n",
    "        files.append(f.name)\n",
    "for f in files:\n",
    "    soup = beau(open(directory + '/' + f).read(), 'html.parser')\n",
    "    fics_df = pd.concat([fics_df, extract_info(soup)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. create additional data columns  \n",
    "    may move this over to data_organization.ipynb\n",
    "5. pickle dataframe as `fics_df.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fics_df_scraped.pkl', 'wb') as file:\n",
    "    pickle.dump(fics_df, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of        L1            author  \\\n",
       "0     NaN    orphan_account   \n",
       "1     NaN       OffKeyPanda   \n",
       "2     NaN          qamiqami   \n",
       "3     NaN     beautifulpain   \n",
       "4     NaN            RenSoo   \n",
       "...   ...               ...   \n",
       "1127  NaN   Piensodemasiado   \n",
       "1128  NaN        AmericanPi   \n",
       "1129  NaN  sixdollardropout   \n",
       "1130  NaN           Ibbyliv   \n",
       "1131  NaN    Lovegingernuts   \n",
       "\n",
       "                                                  title  \\\n",
       "0                     \\n      Bright Red Lanterns\\n       \n",
       "1                    \\n      ¡Qué estas diciendo!\\n       \n",
       "2                          \\n      Beautiful Mess\\n       \n",
       "3         \\n      take only what you need from me\\n       \n",
       "4                            \\n      Forgiveness.\\n       \n",
       "...                                                 ...   \n",
       "1127                       \\n      Baby Don't Cry\\n       \n",
       "1128                  \\n      Before the Darkness\\n       \n",
       "1129              \\n      A smutty Destiel ficlet\\n       \n",
       "1130              \\n      The children of the sun\\n       \n",
       "1131  \\n      The Personal Journal of Sherlock Holme...   \n",
       "\n",
       "                                                   tags  \\\n",
       "0     [General Audiences, No Archive Warnings Apply,...   \n",
       "1     [Teen And Up Audiences, Creator Chose Not To U...   \n",
       "2     [General Audiences, Creator Chose Not To Use A...   \n",
       "3     [Not Rated, Creator Chose Not To Use Archive W...   \n",
       "4     [Teen And Up Audiences, Creator Chose Not To U...   \n",
       "...                                                 ...   \n",
       "1127  [Not Rated, Creator Chose Not To Use Archive W...   \n",
       "1128  [General Audiences, No Archive Warnings Apply,...   \n",
       "1129  [Teen And Up Audiences, Creator Chose Not To U...   \n",
       "1130  [Explicit, Creator Chose Not To Use Archive Wa...   \n",
       "1131  [General Audiences, Creator Chose Not To Use A...   \n",
       "\n",
       "                                                summary  \\\n",
       "0     \\nSummary:\\n\\nFor Prompt: [ Mid-Autumn Festiva...   \n",
       "1     \\nSummary:\\n\\nAlso, know as the one in which C...   \n",
       "2     \\nSummary:\\n\\nJavi finds Yuzu alone in a terra...   \n",
       "3     \\nSummary:\\n\\nHarry's hopelessly in love and L...   \n",
       "4     \\nSummary:\\n\\n3rd part to \"Death, Revival and ...   \n",
       "...                                                 ...   \n",
       "1127  \\nSummary:\\n\\nChanyeol closed his eyes and too...   \n",
       "1128  \\nSummary:\\n\\nA headcanon one-shot that sheds ...   \n",
       "1129  \\nSummary:\\n\\nDean and Castiel was visiting De...   \n",
       "1130  \\nSummary:\\n\\nIt was Aristophanes who said it ...   \n",
       "1131  \\nSummary:\\n\\nSherlock Holmes keeps a diary wh...   \n",
       "\n",
       "                                                  notes  \\\n",
       "0      \\nNotes:\\n(See the end of the work for notes.)\\n   \n",
       "1     \\nNotes:\\n\\nIt was going to be smut but after ...   \n",
       "2     \\nNotes:\\n\\nAnd idea came to me and I thought:...   \n",
       "3     \\nNotes:\\n\\nShort intro to the story I want to...   \n",
       "4     \\nNotes:\\n\\nOk, so i poured my blood, sweat an...   \n",
       "...                                                 ...   \n",
       "1127  \\nNotes:\\n\\nSo yeah, this is my first chanbaek...   \n",
       "1128  \\nNotes:\\n\\nThis is very old - it was written ...   \n",
       "1129  \\nNotes:\\n\\nThis is an AU ficlet where Dean's ...   \n",
       "1130  \\nNotes:\\n\\nPlease excuse all my possible mist...   \n",
       "1131  \\nNotes:\\n\\nOBS! English isn't my first langua...   \n",
       "\n",
       "                                               endnotes  \\\n",
       "0     \\nNotes:\\nI'm Chinese, so writing this brought...   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3     \\nNotes:\\nIt's short. But I will continue if y...   \n",
       "4     \\nNotes:\\nLiterally blood, sweat and tears. Bl...   \n",
       "...                                                 ...   \n",
       "1127                                                NaN   \n",
       "1128  \\nNotes:\\nHeather has been confirmed to be Asi...   \n",
       "1129                                                NaN   \n",
       "1130  \\nNotes:\\n\\nI know that the Hemingway quote \"P...   \n",
       "1131                                                NaN   \n",
       "\n",
       "                                                   work  \\\n",
       "0     [[, ', K, e, v, i, n,  , s, i, g, h, e, d,  , ...   \n",
       "1     [[, \", C, l, a, y,  , l, a, i, d,  , o, n,  , ...   \n",
       "2     [[, \", T, h, e,  , n, i, g, h, t,  , w, a, s, ...   \n",
       "3     [[, \", A, n,  , e, m, p, t, y,  , b, a, r, ,, ...   \n",
       "4     [[, ', \\, n, N, o, w,  , s, i, n, c, e,  , e, ...   \n",
       "...                                                 ...   \n",
       "1127  \\nWork Text:\\nChanyeol turned off the engine o...   \n",
       "1128  \\nWork Text:\\n\\n\"You know, you don't always ha...   \n",
       "1129  \\nWork Text:\\nDean had put Castiel inside the ...   \n",
       "1130  \\n\\n\\n\\n\\n\\nChapter 1: Does it almost feel lik...   \n",
       "1131  \\n\\n\\n\\n\\n\\nChapter 1: A Study in Pink\\n    \\n...   \n",
       "\n",
       "                             rating  \\\n",
       "0         [[], [General Audiences]]   \n",
       "1     [[], [Teen And Up Audiences]]   \n",
       "2         [[], [General Audiences]]   \n",
       "3                 [[], [Not Rated]]   \n",
       "4     [[], [Teen And Up Audiences]]   \n",
       "...                             ...   \n",
       "1127              [[], [Not Rated]]   \n",
       "1128      [[], [General Audiences]]   \n",
       "1129  [[], [Teen And Up Audiences]]   \n",
       "1130               [[], [Explicit]]   \n",
       "1131      [[], [General Audiences]]   \n",
       "\n",
       "                                               warnings  ...  \\\n",
       "0      [[Archive Warning], [No Archive Warnings Apply]]  ...   \n",
       "1     [[Archive Warning], [Creator Chose Not To Use ...  ...   \n",
       "2     [[Archive Warning], [Creator Chose Not To Use ...  ...   \n",
       "3     [[Archive Warning], [Creator Chose Not To Use ...  ...   \n",
       "4     [[Archive Warnings], [Creator Chose Not To Use...  ...   \n",
       "...                                                 ...  ...   \n",
       "1127  [[Archive Warning], [Creator Chose Not To Use ...  ...   \n",
       "1128   [[Archive Warning], [No Archive Warnings Apply]]  ...   \n",
       "1129  [[Archive Warning], [Creator Chose Not To Use ...  ...   \n",
       "1130  [[Archive Warning], [Creator Chose Not To Use ...  ...   \n",
       "1131  [[Archive Warning], [Creator Chose Not To Use ...  ...   \n",
       "\n",
       "                                                 charas  \\\n",
       "0                        [[], [Samandriel, Kevin Tran]]   \n",
       "1        [[], [Clay Jensen, Tony Padilla, Skye Miller]]   \n",
       "2                [[], [Yuzuru Hanyu, Javier Fernández]]   \n",
       "3     [[], [Harry Styles, Louis Tomlinson, Liam Payn...   \n",
       "4     [[], [Kuro | Sleepy Ash, Shirota Mahiru, Arisu...   \n",
       "...                                                 ...   \n",
       "1127               [[], [Byun Baekhyun, Park Chanyeol]]   \n",
       "1128                      [[], [Heather (Total Drama)]]   \n",
       "1129                   [[], [Castiel, Dean Winchester]]   \n",
       "1130  [[], [Enjolras (Les Misérables), Grantaire (Le...   \n",
       "1131  [[], [Sherlock Holmes, John Watson, Molly Hoop...   \n",
       "\n",
       "                                               freeform published status  \\\n",
       "0                [[], [mid-autumn festival, Festivals]]     ERROR  ERROR   \n",
       "1     [[], [Based on a Tumblr Post, Bad Spanish, Eve...     ERROR  ERROR   \n",
       "2     [[], [yuzuvier - Freeform, blb, My english is ...     ERROR  ERROR   \n",
       "3     [[], [Eleanor Is A Beard, Nick doesn't love Ha...     ERROR  ERROR   \n",
       "4     [[], [Forgiveness!, Mahiru gets that second ch...     ERROR  ERROR   \n",
       "...                                                 ...       ...    ...   \n",
       "1127  [[], [Angst, not really platonic, but not real...     ERROR  ERROR   \n",
       "1128  [[], [Angst, Headcanon, Cross-Post, Cross-Post...     ERROR  ERROR   \n",
       "1129  [[], [Blow Jobs, Dirty Talk, Light BDSM, Dom/s...     ERROR  ERROR   \n",
       "1130  [[], [Reincarnation, Fluff, Angst, Hurt/Comfor...     ERROR  ERROR   \n",
       "1131  [[], [Diary/Journal, thoughts, POV Sherlock Ho...     ERROR  ERROR   \n",
       "\n",
       "     word_count  chaps comments kudos hits bookmarks  \n",
       "0         ERROR  ERROR        0     0    0         0  \n",
       "1         ERROR  ERROR        0     0    0         0  \n",
       "2         ERROR  ERROR        0     0    0         0  \n",
       "3         ERROR  ERROR        0     0    0         0  \n",
       "4         ERROR  ERROR        0     0    0         0  \n",
       "...         ...    ...      ...   ...  ...       ...  \n",
       "1127      ERROR  ERROR        0     0    0         0  \n",
       "1128      ERROR  ERROR        0     0    0         0  \n",
       "1129      ERROR  ERROR        0     0    0         0  \n",
       "1130      ERROR  ERROR        0     0    0         0  \n",
       "1131      ERROR  ERROR        0     0    0         0  \n",
       "\n",
       "[1132 rows x 23 columns]>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fics_df.describe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
