{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# file set-up\n",
    "1. import necessary libraries  \n",
    "2. title directory (where data files are stored)  \n",
    "3. create empty dataframe  \n",
    "4. create list of languages\n",
    "    will return to this---currently it doesn't actually serve a purpose, as I was unable to get the languages through here.  \n",
    "    will either update this file or change spider to sort by language.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as beau\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import pickle\n",
    "import nltk\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'natfinder/natfinder/scraped-works'\n",
    "fics_df = pd.DataFrame(columns=['L1', 'author', 'title', 'tags', 'summary', 'notes', 'endnotes', 'work', 'rating', 'warnings', 'category', 'fandom', 'ships', 'charas', 'freeform'])\n",
    "langs = ['afrikaans', 'albanian', 'amharic', 'anii', 'arabic', 'araona', 'armenian', 'assamese', 'aymara', 'ayoreo', 'azerbaijan', 'balanta', 'bambara', 'bariba', 'basque', 'bassari', 'baure', 'bedik', 'belarusian', 'bengali', 'berber', 'biali', 'bislama', 'boko', 'bomu', 'bosnian', 'bozo', 'buduma', 'bulgarian', 'burmese', 'canichana', 'cantonese', 'carolinian', 'catalan', 'cayubaba', 'chacobo', 'chamorro', 'chichewa', 'chinese', 'chirbawe', 'comorian', 'corsican', 'creole', 'croation', 'czech', 'dagaare', 'dagbani', 'dangme', 'danish', 'dari', 'daroese', 'dendi', 'dhivehi', 'dioula', 'dogon', 'dutch', 'estonian', 'fante', 'figian', 'filipino', 'finnish', 'foodo', 'formosan', 'french', 'fula', 'gaelic', 'gbe', 'georgian', 'german', 'gonja', 'gourmanche', 'greek', 'guarani', 'guarayu', 'gujarati', 'hakka', 'hassaniya', 'hausa', 'hebrew', 'hindi', 'hiri', 'hokkien', 'hungarian', 'icelandic', 'igbo', 'indonesian', 'irish', 'italian', 'itene', 'itonama', 'japanese', 'javanese', 'jerriais', 'jola', 'kabye', 'kalanga', 'kallawaya', 'kannada', 'kanuri', 'kasem', 'kazakh', 'khmer', 'kinyarwanda', 'kirundi', 'kissi', 'koisan', 'korean', 'kpelle', 'kurdish', 'kyrgyz', 'lao', 'latvian', 'leco', 'lithuanian', 'lukpa', 'luzembourgish', 'macedonian', 'malagasy', 'malay', 'malayalam', 'malinke', 'maltese', 'mamara', 'mandarin', 'manding', 'mandinka', 'mandjak', 'manipuri', 'mankanya', 'maori', 'marathi', 'marshallese', 'mbelime', 'meitei', 'mongolian', 'montenegrin', 'moseten', 'mossi', 'motu', 'movima', 'moxos', 'nambya', 'nateni', 'nauruan', 'ndau', 'ndebele', 'nepali', 'norwegian', 'nzema', 'oniyan', 'oriya', 'oromo', 'ossetian', 'pakawara', 'palauan', 'papiamento', 'pashto', 'persian', 'pisin', 'polish', 'portuguese', 'punjabi', 'puquina', 'quechua', 'romanian', 'romansh', 'russian', 'safen', 'sango', 'scots', 'scottish', 'sena', 'serbian', 'serer', 'sewdish', 'shinhala', 'shona', 'siriono', 'slovak', 'slovene', 'somali', 'soninke', 'sonsorolese', 'sotho', 'spanish', 'susu', 'swahili', 'swati', 'syenara', 'tacana', 'tagalog', 'tajik', 'tamasheq', 'tamil', 'tammari', 'tapiete', 'tasawaq', 'tebu', 'telugu', 'tetum', 'thai', 'tigrinya', 'tobian', 'toma', 'tonga', 'tongan', 'toromono', 'tsonga', 'tswana', 'turkish', 'turkmen', 'tuvaluan', 'twi', 'ukrainian', 'urdu', 'uzbek', 'venda', 'vietnamese', 'waama', 'wamey', 'weenhayek', 'welsh', 'wolof', 'xhosa', 'yaminawa', 'yobe', 'yom', 'yoruba', 'yuki', 'yuracare', 'zarma', 'zulu']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# def: extract_info(soup)\n",
    "1. try/except in case data is missing from file (ie; fics written by anonymous users will not return an author value)  \n",
    "2. gather basic info (author, title, tags, summary, notes, endnotes, work)  \n",
    "    tag info (rating, warning, category, fandom, (relation)ships, characters, freeform)  \n",
    "    stat info (publication date, status, word count, chapters, comments, kudos, hits, bookmarks)\n",
    "3. add to `fics_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(soup):\n",
    "    # basic info\n",
    "    try:\n",
    "        author = soup.find(rel='author').text\n",
    "    except:\n",
    "        author = 'NaN'\n",
    "    try:\n",
    "        title = soup.find(class_='title heading').text\n",
    "    except:\n",
    "        title = 'NaN'\n",
    "    try:\n",
    "        tags = [t.text for t in soup.find_all(class_='tag')]\n",
    "    except:\n",
    "        tags = 'NaN'\n",
    "    try:\n",
    "        summary = soup.find('div', class_='summary module').text\n",
    "    except:\n",
    "        summary = 'NaN'\n",
    "    try:\n",
    "        notes = soup.find('div', class_='notes module').text\n",
    "    except:\n",
    "        notes = 'NaN'\n",
    "    try:\n",
    "        endnotes = soup.find('div', class_='end notes module').text\n",
    "    except:\n",
    "        endnotes = 'NaN'\n",
    "    try:\n",
    "        work = soup.text\n",
    "    except:\n",
    "        work = 'NaN'\n",
    "    # tag info\n",
    "    try:\n",
    "        rating = [[a.text for a in t.find_all('a')] for t in soup.find_all(class_='rating tags')]\n",
    "    except:\n",
    "        rating = 'ERROR'\n",
    "    try:\n",
    "        warnings = [[a.text for a in t.find_all('a')] for t in soup.find_all(class_='warning tags')]\n",
    "    except:\n",
    "        warnings = 'ERROR'\n",
    "    try:\n",
    "        categories = [[a.text for a in t.find_all('a')] for t in soup.find_all(class_='category tags')]\n",
    "    except:\n",
    "        categories = 'ERROR'\n",
    "    try:\n",
    "        fandoms = [[a.text for a in t.find_all('a')] for t in soup.find_all(class_='fandom tags')]\n",
    "    except:\n",
    "        fandoms = 'ERROR'\n",
    "    try:\n",
    "        ships = [[a.text for a in t.find_all('a')] for t in soup.find_all(class_='relationship tags')]\n",
    "    except:\n",
    "        ships = 'NaN'\n",
    "    try:\n",
    "        charas = [[a.text for a in t.find_all('a')] for t in soup.find_all(class_='character tags')]\n",
    "    except:\n",
    "        charas = 'NaN'\n",
    "    try:\n",
    "        freeform = [[a.text for a in t.find_all('a')] for t in soup.find_all(class_='freeform tags')]\n",
    "    except:\n",
    "        freeform = 'NaN'\n",
    "    temp = pd.DataFrame({'author':[author], 'title':[title],'tags':[tags], 'summary':[summary], 'notes':[notes], 'endnotes':[endnotes], 'work':[work], 'rating':[rating], 'warnings':[warnings], 'category':[categories], 'fandom':[fandoms], 'ships':[ships], 'charas':[charas], 'freeform':[freeform]})\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parsing through files\n",
    "1. gather files  \n",
    "2. create bs4 soup for files\n",
    "3. use `extract_info(soup)` to add to fics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for f in os.scandir(directory):\n",
    "    if f.name.endswith('.html'):\n",
    "        files.append(f.name)\n",
    "for f in files:\n",
    "    soup = beau(open(directory + '/' + f).read(), 'html.parser')\n",
    "    fics_df = pd.concat([fics_df, extract_info(soup)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. clean up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fics_df['work'] = fics_df.work.map(lambda x : re.sub(r'(?s).*Work Text:', '', re.sub(r'(?s)\\xa0\\n+.*', '', re.sub(r'\\s', ' ', x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. pickle dataframe as `fics_df_scraped.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('fics_df_scraped.pkl', 'wb') as file:\n",
    "    #pickle.dump(fics_df, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. come back and find L1 of author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fics_df_scraped.pkl', 'rb') as file:\n",
    "    fics_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lang(fic):\n",
    "    for l in langs:\n",
    "        if l in nltk.word_tokenize(fic.lower()):\n",
    "            return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lang_tags(fic):\n",
    "    for l in langs:\n",
    "        for x in fic:\n",
    "            if l in nltk.word_tokenize(x.lower()):\n",
    "                return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "fics_df['L1s'] = fics_df.summary.map(lambda x : find_lang(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "fics_df['L1n'] = fics_df.notes.map(lambda x : find_lang(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "fics_df['L1e'] = fics_df.endnotes.map(lambda x : find_lang(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "fics_df['L1t'] = fics_df.tags.map(lambda x : find_lang_tags(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "fics_df['L1'] = fics_df[['L1s', 'L1n', 'L1e', 'L1t']].bfill(axis=1).iloc[:, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "fics_df.drop(columns=['L1s', 'L1n', 'L1e', 'L1t'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "fics_df_lang = fics_df[fics_df['L1'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "fics_df_mys = fics_df[fics_df['L1'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fics_df_scraped_lang.pkl', 'wb') as file:\n",
    "    pickle.dump(fics_df_lang, file)\n",
    "with open('fics_df_scraped_mys.pkl', 'wb') as file:\n",
    "    pickle.dump(fics_df_mys, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
