{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# file set-up\n",
    "1. import necessary libraries  \n",
    "2. title directory (where data files are stored)  \n",
    "3. create empty dataframe  \n",
    "4. create list of languages\n",
    "    will return to this---currently it doesn't actually serve a purpose, as I was unable to get the languages through here.  \n",
    "    will either update this file or change spider to sort by language.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup as beau\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'natfinder'\n",
    "fics_df = pd.DataFrame(columns=['lang', 'author', 'title', 'file', 'tags', 'summary', 'notes', 'endnotes', 'work'])\n",
    "langs = ['french', 'spanish', 'mexican', 'brazilian', 'portugeuse', 'russian', 'ukranian', 'bengali', 'italian', 'czech', 'japanese', 'korean', 'chinese', 'swedish', 'german', 'finnish', 'turkish', 'greek', 'hindi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# def: extract_info(soup)\n",
    "1. try/except in case data is missing from file (ie; fics written by anonymous users will not return an author value)  \n",
    "2. gather author, title, tags, summary, notes, endnotes, work  \n",
    "    may come back to separate tags into rating, warning, fandom, characters, relationships, freeform tags  \n",
    "3. add to `fics_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(soup, f):\n",
    "    try:\n",
    "        author = soup.find(rel='author').text\n",
    "    except:\n",
    "        author = 'NaN'\n",
    "    try:\n",
    "        title = soup.find(class_='title heading').text\n",
    "    except:\n",
    "        title = 'NaN'\n",
    "    try:\n",
    "        tags = [t.text for t in soup.find_all(class_='tag')]\n",
    "    except:\n",
    "        tags = 'NaN'\n",
    "    try:\n",
    "        summary = soup.find('div', class_='summary module').text\n",
    "    except:\n",
    "        summary = 'NaN'\n",
    "    try:\n",
    "        notes = soup.find('div', class_='notes module').text\n",
    "    except:\n",
    "        notes = 'NaN'\n",
    "    try:\n",
    "        endnotes = soup.find('div', class_='end notes module').text\n",
    "    except:\n",
    "        endnotes = 'NaN'\n",
    "    try:\n",
    "        work = str([p.text for p in soup.div.find(role='article').find_all('p')])\n",
    "    except:\n",
    "        work = 'NaN'\n",
    "    temp = pd.DataFrame({'author':[author], 'title':[title], 'file':[f], 'tags':[tags], 'summary':[summary], 'notes':[notes], 'endnotes':[endnotes], 'work':[work]})\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parsing through files\n",
    "1. gather files  \n",
    "2. create bs4 soup for files\n",
    "3. use `extract_info(soup)` to add to fics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for f in os.scandir(directory):\n",
    "    if f.name.startswith('work'):\n",
    "        files.append(f.name)\n",
    "for f in files:\n",
    "    soup = beau(open('natfinder/' + f).read(), 'html.parser')\n",
    "    fics_df = pd.concat([fics_df, extract_info(soup, f)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. create additional data columns  \n",
    "    may move this over to data_organization.ipynb\n",
    "5. pickle dataframe as `fics_df.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fics_df['sum_toks'] = fics_df.summary.map(lambda x: nltk.word_tokenize(x)[2:])\n",
    "fics_df['notes_toks'] = fics_df.notes.map(lambda x: nltk.word_tokenize(x)[2:])\n",
    "fics_df['endnotes_toks'] = fics_df.endnotes.map(lambda x: nltk.word_tokenize(x)[2:])\n",
    "fics_df['toks'] = fics_df.work.map(lambda x: nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fics_df.pkl', 'wb') as file:\n",
    "    pickle.dump(fics_df, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
